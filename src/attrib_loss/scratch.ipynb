{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# flake8: noqa\n",
                "# %%\n",
                "import sys\n",
                "sys.path.append(\"/workspace/smol-sae\")\n",
                "import torch.nn.functional as F\n",
                "import transformer_lens as tl\n",
                "import sae_lens as sl\n",
                "from functools import partial\n",
                "from torch import Tensor\n",
                "from jaxtyping import Float\n",
                "from smol_sae.base import Config\n",
                "from smol_sae.utils import get_splits\n",
                "from smol_sae.vanilla import VanillaSAE\n",
                "\n",
                "from datasets import load_dataset \n",
                "\n",
                "# define loss function\n",
                "\n",
                "# x [batch, d_model]\n",
                "# grad_sae_acts [batch, ]\n",
                "# assume gradients have a batch dim\n",
                "\n",
                "def loss(\n",
                "    sae, \n",
                "    x: Float[Tensor, \"batch d_model\"], \n",
                "    x_rec:  Float[Tensor, \"batch d_model\"], \n",
                "    sae_acts:  Float[Tensor, \"batch d_sae\"], \n",
                "    # backward hook\n",
                "    grad_sae_acts:  Float[Tensor, \"batch d_sae\"],\n",
                "    grad_x: Float[Tensor, \"batch d_model\"],\n",
                "    lamda: float = 1.0,\n",
                "    alpha: float = 1.0,\n",
                "    beta: float = 1.0,\n",
                "):\n",
                "    # reconstruction term \n",
                "    l2_loss = (x-x_rec).square().sum()\n",
                "    l1_loss = sae_acts.abs().sum()\n",
                "    attr_loss = (sae_acts * grad_sae_acts).abs().sum()\n",
                "    unexplained_loss = ((x-x_rec) * grad_x).abs().sum()\n",
                "    \n",
                "    return (\n",
                "        l2_loss \n",
                "        + lamda * l1_loss \n",
                "        + alpha * attr_loss \n",
                "        + beta * unexplained_loss\n",
                "    )\n",
                "# %%\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/workspace/hacking/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded pretrained model gelu-1l into HookedTransformer\n",
                        "hook_embed\n",
                        "hook_pos_embed\n",
                        "blocks.0.ln1.hook_scale\n",
                        "blocks.0.ln1.hook_normalized\n",
                        "blocks.0.ln2.hook_scale\n",
                        "blocks.0.ln2.hook_normalized\n",
                        "blocks.0.attn.hook_k\n",
                        "blocks.0.attn.hook_q\n",
                        "blocks.0.attn.hook_v\n",
                        "blocks.0.attn.hook_z\n",
                        "blocks.0.attn.hook_attn_scores\n",
                        "blocks.0.attn.hook_pattern\n",
                        "blocks.0.attn.hook_result\n",
                        "blocks.0.mlp.hook_pre\n",
                        "blocks.0.mlp.hook_post\n",
                        "blocks.0.hook_attn_in\n",
                        "blocks.0.hook_q_input\n",
                        "blocks.0.hook_k_input\n",
                        "blocks.0.hook_v_input\n",
                        "blocks.0.hook_mlp_in\n",
                        "blocks.0.hook_attn_out\n",
                        "blocks.0.hook_mlp_out\n",
                        "blocks.0.hook_resid_pre\n",
                        "blocks.0.hook_resid_mid\n",
                        "blocks.0.hook_resid_post\n",
                        "ln_final.hook_scale\n",
                        "ln_final.hook_normalized\n"
                    ]
                }
            ],
            "source": [
                "device = \"cuda\"\n",
                "model = tl.HookedTransformer.from_pretrained(\"gelu-1l\")\n",
                "for hook_point in model.hook_points():\n",
                "    print(hook_point.name)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "42dacb8716d24db7a4c1261bdb4042dd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Resolving data files:   0%|          | 0/23 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[{'tokens': tensor([    1,   769, 41811,  ...,  6109,  1849,   360])}, {'tokens': tensor([    1,  2009,  7402,  ...,   825,   274, 13085])}, {'tokens': tensor([    1,  1582,  7537,  ...,  2411,  5379, 12147])}, {'tokens': tensor([    1,  1056,   274,  ...,  7538, 10523,   274])}, {'tokens': tensor([    1, 23947,  9863,  ...,   274,   254,  4237])}, {'tokens': tensor([    1,  7732, 13492,  ...,   368, 15813,   772])}, {'tokens': tensor([    1,  1391,  1209,  ...,   282, 13905,   276])}, {'tokens': tensor([   1,  282, 6134,  ...,  254, 7736, 2832])}, {'tokens': tensor([    1,   300,    15,  ..., 15543,    14,   390])}, {'tokens': tensor([   1, 3056,   16,  ...,  368,  670, 3202])}, {'tokens': tensor([   1, 3983,  328,  ...,  618,  801,  671])}, {'tokens': tensor([    1,     0,  7345,  ...,   254,  7885, 19282])}, {'tokens': tensor([    1,   286,  1006,  ..., 44292,    16,   380])}, {'tokens': tensor([    1, 11444,   407,  ...,    25,    28,  2004])}, {'tokens': tensor([    1, 10077,   479,  ..., 28101,   324,  4090])}, {'tokens': tensor([    1,  2133,   274,  ..., 41913,  7846,   282])}, {'tokens': tensor([   1, 2515,  368,  ..., 3629,  311, 8418])}, {'tokens': tensor([    1,    16,   310,  ...,   254,  4785, 22468])}, {'tokens': tensor([  1,  14, 835,  ...,  26,  18,  18])}, {'tokens': tensor([    1,    18,    16,  ...,   282, 31420,    69])}, {'tokens': tensor([   1,  348,  944,  ...,  668,  254, 1599])}, {'tokens': tensor([   1,  619,  669,  ..., 6255,  286, 6384])}, {'tokens': tensor([   1,   38, 7939,  ..., 2228, 3831,  248])}, {'tokens': tensor([   1,  321, 3265,  ...,  512, 2377,  867])}, {'tokens': tensor([    1,  5798,   362,  ..., 16466, 11171,   351])}, {'tokens': tensor([   1, 2109,   14,  ...,   14,  286,  596])}, {'tokens': tensor([   1, 1379,  671,  ...,  603,  368, 1038])}, {'tokens': tensor([   1, 4179,  286,  ...,  457,   85, 2687])}, {'tokens': tensor([    1,  2943,   368,  ...,  1934,  3227, 16881])}, {'tokens': tensor([  1, 248, 745,  ..., 336, 613,  14])}, {'tokens': tensor([   1,   10, 1251,  ..., 3567,   16,  314])}, {'tokens': tensor([    1, 12495,  2186,  ...,   274,   427, 12654])}]\n"
                    ]
                }
            ],
            "source": [
                "train_dataset = load_dataset(\n",
                "    \"NeelNanda/c4-tokenized-2b\", split=\"train\", streaming=True\n",
                ").with_format(\"torch\")\n",
                "train_batch = list(train_dataset.take(32))\n",
                "print(train_batch)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = Config(\n",
                "    n_buffers=100, expansion=4, buffer_size=2**8, sparsities=(0.1, 1.0), device=device\n",
                ")\n",
                "sae = VanillaSAE(config, model)\n",
                "print(sae.d_model)\n",
                "print(model.cfg.d_model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'sae' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m     gradients[hook\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m grad_act\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_act\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[0;32m---> 16\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.mlp.hook_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, partial(fwd_patch_model_with_sae, sae\u001b[38;5;241m=\u001b[39m\u001b[43msae\u001b[49m))],\n\u001b[1;32m     17\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.mlp.hook_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, bwd_patch_model_gradient)],\n\u001b[1;32m     18\u001b[0m ):\n\u001b[1;32m     19\u001b[0m     model(train_batch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# sae(train_batch[0][\"input_ids\"])\u001b[39;00m\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'sae' is not defined"
                    ]
                }
            ],
            "source": [
                "gradients = {}\n",
                "\n",
                "def fwd_patch_model_with_sae(act, hook, sae):\n",
                "    sae_out, hidden = sae(act)[:2]\n",
                "    sae_err = act - sae_out.detach()\n",
                "    return sae_out + sae_err \n",
                "\n",
                "def bwd_patch_model_gradient(grad_act, hook):\n",
                "    global gradients \n",
                "    gradients[hook.name] = grad_act.detach()\n",
                "    return grad_act\n",
                "\n",
                "\n",
                "\n",
                "with model.hooks(\n",
                "    fwd_hooks=[(\"blocks.0.mlp.hook_pre\", partial(fwd_patch_model_with_sae, sae=sae))],\n",
                "    bwd_hooks=[(\"blocks.0.mlp.hook_pre\", bwd_patch_model_gradient)],\n",
                "):\n",
                "    model(train_batch[0][\"tokens\"].to(device))\n",
                "    # sae(train_batch[0][\"input_ids\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
