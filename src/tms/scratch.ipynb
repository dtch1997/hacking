{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "project_dir = pathlib.Path('/workspace/hacking')\n",
    "# TODO: Add callum's directory to the system path\n",
    "sys.path.append(str(project_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flake8: noqa\n",
    "# %%\n",
    "import torch as t\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass, field\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from typing import Optional, Union, Callable\n",
    "\n",
    "t.manual_seed(2)\n",
    "\n",
    "W = t.randn(2, 5)\n",
    "W_normed = W / W.norm(dim=0, keepdim=True)\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def constant_lr(*_):\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # We optimize n_instances models in a single training loop to let us sweep over\n",
    "    # sparsity or importance curves  efficiently. You should treat `n_instances` as\n",
    "    # kinda like a batch dimension, but one which is built into our training setup.\n",
    "    n_instances: int\n",
    "    # n_features: int = 5\n",
    "    groups: list[int] = field(default_factory=lambda: [2, 2])\n",
    "    n_hidden: int = 2\n",
    "\n",
    "    @property\n",
    "    def n_features(self) -> int:\n",
    "        return sum(self.groups)\n",
    "\n",
    "\n",
    "# Toy model of superposition\n",
    "class Model(nn.Module):\n",
    "    W: Float[Tensor, \"n_instances n_hidden n_features\"]\n",
    "    b_final: Float[Tensor, \"n_instances n_features\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        feature_probability: Optional[Union[float, Tensor]] = None,\n",
    "        importance: Optional[Union[float, Tensor]] = None,\n",
    "        device=device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        n_features = sum(cfg.groups)\n",
    "        self.n_features = n_features\n",
    "\n",
    "        if feature_probability is None:\n",
    "            feature_probability = t.ones(())\n",
    "        if isinstance(feature_probability, float):\n",
    "            feature_probability = t.tensor(feature_probability)\n",
    "        self.feature_probability = feature_probability.to(device).broadcast_to(\n",
    "            (cfg.n_instances, n_features)\n",
    "        )\n",
    "        if importance is None:\n",
    "            importance = t.ones(())\n",
    "        if isinstance(importance, float):\n",
    "            importance = t.tensor(importance)\n",
    "        self.importance = importance.to(device).broadcast_to(\n",
    "            (cfg.n_instances, n_features)\n",
    "        )\n",
    "\n",
    "        self.W = nn.Parameter(\n",
    "            nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden, n_features)))\n",
    "        )\n",
    "        self.b_final = nn.Parameter(t.zeros((cfg.n_instances, n_features)))\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(\n",
    "        self, features: Float[Tensor, \"... instances features\"]\n",
    "    ) -> Float[Tensor, \"... instances features\"]:\n",
    "        hidden = einops.einsum(\n",
    "            features,\n",
    "            self.W,\n",
    "            \"... instances features, instances hidden features -> ... instances hidden\",\n",
    "        )\n",
    "        out = einops.einsum(\n",
    "            hidden,\n",
    "            self.W,\n",
    "            \"... instances hidden, instances hidden features -> ... instances features\",\n",
    "        )\n",
    "        return F.relu(out + self.b_final)\n",
    "\n",
    "    def generate_batch(\n",
    "        self, batch_size\n",
    "    ) -> Float[Tensor, \"batch_size instances features\"]:\n",
    "        \"\"\"\n",
    "        Generates a batch of data. We'll return to this function later when we apply correlations.\n",
    "        \"\"\"\n",
    "        return torch.cat(\n",
    "            [\n",
    "                F.one_hot(\n",
    "                    torch.randint(\n",
    "                        0,\n",
    "                        i,\n",
    "                        (\n",
    "                            batch_size,\n",
    "                            self.cfg.n_instances,\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "                for i in self.cfg.groups\n",
    "            ],\n",
    "            dim=-1,\n",
    "        ).to(device) * 1.0\n",
    "\n",
    "    def calculate_loss(\n",
    "        self,\n",
    "        out: Float[Tensor, \"batch instances features\"],\n",
    "        batch: Float[Tensor, \"batch instances features\"],\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss for a given batch, using this loss described in the Toy Models paper:\n",
    "\n",
    "            https://transformer-circuits.pub/2022/toy_model/index.html#demonstrating-setup-loss\n",
    "\n",
    "        Remember, `self.importance` will always have shape (n_instances, n_features).\n",
    "        \"\"\"\n",
    "        error = self.importance * ((batch - out) ** 2)\n",
    "        loss = einops.reduce(\n",
    "            error, \"batch instances features -> instances\", \"mean\"\n",
    "        ).sum()\n",
    "        return loss\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        batch_size: int = 1024,\n",
    "        steps: int = 10_000,\n",
    "        log_freq: int = 100,\n",
    "        lr: float = 1e-3,\n",
    "        lr_scale: Callable[[int, int], float] = constant_lr,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Optimizes the model using the given hyperparameters.\n",
    "        \"\"\"\n",
    "        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n",
    "\n",
    "        progress_bar = tqdm(range(steps))\n",
    "\n",
    "        for step in progress_bar:\n",
    "            # Update learning rate\n",
    "            step_lr = lr * lr_scale(step, steps)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = step_lr\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            batch = self.generate_batch(batch_size)\n",
    "            out = self(batch)\n",
    "            loss = self.calculate_loss(out, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display progress bar\n",
    "            if step % log_freq == 0 or (step + 1 == steps):\n",
    "                progress_bar.set_postfix(\n",
    "                    loss=loss.item() / self.cfg.n_instances, lr=step_lr\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:07<00:00, 141.15it/s, loss=0.026, lr=0.001]\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(n_instances=8, n_hidden=2, groups=[2, 2])\n",
    "model = Model(cfg)\n",
    "model.optimize(steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 1., 0., 1., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 1., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 1., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 1., 0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0., 1., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 1., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 1., 0., 0., 1.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "cfg = Config(n_instances=8, n_hidden=2, groups=[3, 3, 3])\n",
    "model = Model(cfg)\n",
    "print(model.generate_batch(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
